{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEfmz4N02KW3VswkYqsJ7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinjiroTsuku/Ani-Aide_Hackathon/blob/main/Estimate_GPT_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Hfpz79YYUy",
        "outputId": "7b4487a2-2704-4d7d-f7fb-789df5cde957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/研究室/解析データ/M1/本実験\n",
            "処理開始時刻: 2025-12-24 15:47:50\n"
          ]
        }
      ],
      "source": [
        "# Google Colabドライブをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "\n",
        "\n",
        "# ディレクトリ移動（本実験に変更）\n",
        "%cd /content/drive/MyDrive/研究室/解析データ/M1/本実験\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from google.colab import userdata\n",
        "\n",
        "# 実行時間計測開始\n",
        "start_time = time.time()\n",
        "start_datetime = datetime.now()\n",
        "print(f\"処理開始時刻: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# 機械学習関連\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score,\n",
        "                            recall_score, f1_score, precision_recall_curve, auc)\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
        "import shap\n",
        "\n",
        "# GPT-4o出力ディレクトリの作成\n",
        "os.makedirs('Output/GPT_Transformer/aupr', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/selected_features', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/analysis', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/analysis/words', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/aupr/individual', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================ 設定 ================\n",
        "experiment_number = 'c04'  # 実験番号（要変更）\n",
        "print(f\"実験番号 {experiment_number} のGPT-4o + Transformer難易度評価分析を開始します...\")\n",
        "\n",
        "# 出力ディレクトリの作成\n",
        "os.makedirs('Output/GPT_Transformer/aupr', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/aupr/individual', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/selected_features', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/analysis', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/analysis/words', exist_ok=True)\n",
        "\n",
        "# OpenAI APIクライアントの初期化\n",
        "try:\n",
        "    client = OpenAI(api_key=userdata.get('API_KEY'))\n",
        "    if client is None:\n",
        "        print(\"⚠️ OpenAI APIキーが未設定です。\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"✅ OpenAI APIクライアントが正常に初期化されました。\")\n",
        "        print(\"使用モデル: gpt-4o + Transformer\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ OpenAI初期化エラー: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 実行時間計測開始\n",
        "start_time = time.time()\n",
        "start_datetime = datetime.now()\n",
        "print(f\"処理開始時刻: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# ================ データの読み込み ================\n",
        "print(\"データを読み込んでいます...\")\n",
        "# 予備実験のLSTM特徴量ファイルから読み込み\n",
        "df = pd.read_json(f'Output/LSTM/features/{experiment_number}-features.json')\n",
        "df = df.dropna()\n",
        "\n",
        "print(f\"データ形状: {df.shape}\")\n",
        "print(f\"未知単語ラベルの分布: {Counter(df['unknownWordLabel'])}\")\n",
        "\n",
        "# ================ lineカラムから文脈を取得 ================\n",
        "print(\"\\nlineカラムから文脈データを取得中...\")\n",
        "\n",
        "# lineカラムが既に存在することを確認\n",
        "if 'line' not in df.columns:\n",
        "    print(\"❌ エラー: 'line'カラムがデータフレームに存在しません。\")\n",
        "    exit()\n",
        "\n",
        "# lineカラムをsentenceとして使用\n",
        "df['sentence'] = df['line']\n",
        "print(f\"文脈取得完了: {len(df)} 単語\")\n",
        "\n",
        "# ================ GPT-4o難易度評価関数 ================\n",
        "def get_gpt_difficulty_score(word, context_sentence, max_retries=3):\n",
        "    \"\"\"gpt-4oを使用して単語の文脈的難易度を1-10の整数値で評価\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an expert English language difficulty assessor for second language learners.\n",
        "\n",
        "Please evaluate the difficulty of the word \"{word}\" in this specific context for an intermediate English learner:\n",
        "\n",
        "Context: \"{context_sentence}\"\n",
        "\n",
        "Consider these factors:\n",
        "- Semantic complexity and abstractness in this context\n",
        "- Collocational patterns and usage constraints\n",
        "\n",
        "Provide a precise integer score between 1 and 10:\n",
        "- 1-2: Very easy (basic vocabulary, high frequency)\n",
        "- 3-4: Easy (common words, straightforward usage)\n",
        "- 5-6: Moderate (intermediate vocabulary, some complexity)\n",
        "- 7-8: Difficult (advanced vocabulary, complex usage)\n",
        "- 9-10: Very difficult (rare, highly technical, or complex)\n",
        "\n",
        "Return ONLY the integer number (e.g., 2, 6, 9). Do not provide explanations.\n",
        "\n",
        "Score:\n",
        "\"\"\"\n",
        "\n",
        "    for retry in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a precise language difficulty assessor. Respond ONLY with an integer number between 1 and 10.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=10,\n",
        "                temperature=0.0,\n",
        "            )\n",
        "\n",
        "            score_text = response.choices[0].message.content.strip()\n",
        "            score_match = re.search(r'\\d+', score_text)\n",
        "\n",
        "            if score_match:\n",
        "                score_int = int(score_match.group())\n",
        "                score_int = max(1, min(10, score_int))\n",
        "                normalized_score = score_int / 10.0\n",
        "                return normalized_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"GPT-4o API エラー (試行 {retry + 1}): {str(e)}\")\n",
        "            if retry < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "\n",
        "    raise Exception(f\"GPT-4o評価に失敗: {word}\")\n",
        "\n",
        "def add_gpt_difficulty_features(data):\n",
        "    \"\"\"データフレームにGPT-4o難易度評価特徴量を追加\"\"\"\n",
        "    gpt_difficulties = []\n",
        "\n",
        "    print(\"gpt-4oによる難易度評価を実行中（新規評価）...\")\n",
        "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
        "        word = row['word']\n",
        "        sentence = row['sentence']\n",
        "\n",
        "        try:\n",
        "            difficulty = get_gpt_difficulty_score(word, sentence)\n",
        "        except Exception as e:\n",
        "            print(f\"評価失敗: {word} - {str(e)}\")\n",
        "            difficulty = 0.5\n",
        "\n",
        "        gpt_difficulties.append(difficulty)\n",
        "\n",
        "    data_with_gpt = data.copy()\n",
        "    data_with_gpt['gpt_difficulty'] = gpt_difficulties\n",
        "\n",
        "    return data_with_gpt\n",
        "\n",
        "# ================ Transformerモデル定義 ================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim=1, d_model=32, nhead=2, num_layers=1,\n",
        "                 dim_feedforward=64, dropout=0.2, pooling='mean'):\n",
        "        super().__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, 2)\n",
        "        self.pooling = pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_dim)\n",
        "        x = self.input_projection(x)  # (batch, seq_len, d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pooling == 'mean':\n",
        "            x_pooled = x.mean(dim=1)  # (batch, d_model)\n",
        "        elif self.pooling == 'max':\n",
        "            x_pooled = x.max(dim=1)[0]\n",
        "        else:  # last\n",
        "            x_pooled = x[:, -1, :]\n",
        "\n",
        "        out = self.fc(x_pooled)\n",
        "        return out, x_pooled\n",
        "\n",
        "# ================ 特徴量の定義 ================\n",
        "baseline_features_original = [\n",
        "    'length', 'freq', 'seven_character', 'ContentWord', 'syllables',\n",
        "    'Num_Words', 'Length_Word_Ave', 'freq_Min', 'freq_Max', 'freq_Ave',\n",
        "    'Num_7Characters', 'Rate_7Characters',\n",
        "    'Num_ContentWords', 'Rate_ContentWords',\n",
        "    'Num_FunctionWords', 'Rate_FunctionWords',\n",
        "    'Num_Monosyllable', 'Num_Polysyllable',\n",
        "    'Flesch_Reading_Ease', 'ARI', 'Readtime', 'ReadBack'\n",
        "]\n",
        "\n",
        "# ベースライン: 従来特徴量 + GPT-4o難易度\n",
        "gpt_features = ['gpt_difficulty']\n",
        "baseline_features = baseline_features_original + gpt_features\n",
        "\n",
        "# 提案手法: ベースライン + Transformer視線特徴量\n",
        "transformer_features = [f'transformer{i}' for i in range(32)]\n",
        "proposed_features = baseline_features + transformer_features\n",
        "\n",
        "print(\"\\n特徴量構成:\")\n",
        "print(f\"  従来特徴量: {len(baseline_features_original)}個\")\n",
        "print(f\"  ベースライン (従来 + GPT-4o): {len(baseline_features)}個\")\n",
        "print(f\"  提案手法 (ベースライン + Transformer): {len(proposed_features)}個\")\n",
        "\n",
        "# ================ ユーティリティ関数 ================\n",
        "def extract_shap_features(X, y, feature_names, n_features=30):\n",
        "    \"\"\"SHAP値を用いて重要な特徴量を抽出\"\"\"\n",
        "    print(f\"SHAP分析により上位{n_features}特徴量を選択中...\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    model = LogisticRegression(class_weight=\"balanced\", max_iter=1000, random_state=0)\n",
        "    model.fit(X_scaled, y)\n",
        "\n",
        "    explainer = shap.Explainer(model, X_scaled)\n",
        "    shap_values = explainer(X_scaled)\n",
        "\n",
        "    mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "    top_idx = np.argsort(mean_abs_shap)[::-1][:n_features]\n",
        "    selected_features = [feature_names[i] for i in top_idx]\n",
        "\n",
        "    print(f\"SHAP選択完了: {len(selected_features)}個の特徴量\")\n",
        "    return selected_features\n",
        "def extract_transformer_features(data, transformer_model, device):\n",
        "    \"\"\"Transformerモデルから特徴量を抽出\"\"\"\n",
        "    transformer_model.eval()\n",
        "    line_features = {}\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        line_id = str(row['id_line'] + 1)\n",
        "        x_seq = row['x_coordinates_dict'].get(line_id)\n",
        "\n",
        "        if x_seq and len(x_seq) > 0:\n",
        "            # LSTMと同じ形式: [[[x_seq]]]\n",
        "            x_tensor = torch.tensor([[[x] for x in x_seq]], dtype=torch.float32).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, features = transformer_model(x_tensor)\n",
        "            line_features[row.name] = features.squeeze(0).cpu().numpy()\n",
        "\n",
        "    return line_features\n",
        "\n",
        "def add_transformer_features_to_data(data, line_features, n_features=32):\n",
        "    \"\"\"データフレームにTransformer特徴量を追加\"\"\"\n",
        "    for i in range(n_features):\n",
        "        data[f'transformer{i}'] = [\n",
        "            line_features[idx][i] if idx in line_features else 0.0\n",
        "            for idx in data.index\n",
        "        ]\n",
        "    return data\n",
        "\n",
        "def feature_selection_sbs(X, y, feature_names):\n",
        "    \"\"\"Sequential Backward Selection (SBS)による特徴量選択\"\"\"\n",
        "    print(\"Sequential Backward Selection (SBS)実行中...\")\n",
        "\n",
        "    clf = SVC(gamma='scale', probability=True, class_weight='balanced', random_state=0)\n",
        "    selector = sfs(clf,\n",
        "                   k_features=(1, len(feature_names)),\n",
        "                   forward=False,\n",
        "                   floating=False,  # Floatingを無効化してSBSに\n",
        "                   scoring='f1',\n",
        "                   cv=list(StratifiedKFold(n_splits=5, shuffle=True, random_state=0).split(X, y)),\n",
        "                   n_jobs=-1)\n",
        "\n",
        "    X_df = pd.DataFrame(X, columns=feature_names)\n",
        "    selector = selector.fit(X_df, y)\n",
        "\n",
        "    selected_features = X_df.columns[list(selector.k_feature_idx_)].tolist()\n",
        "    X_selected = X_df[selected_features].values\n",
        "\n",
        "    print(f\"選択された特徴量数: {len(selected_features)}\")\n",
        "    print(f\"選択された特徴量: {selected_features}\")\n",
        "\n",
        "    return X_selected, selected_features\n",
        "\n",
        "def train_and_predict(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"SVMモデルの訓練と予測\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    clf = SVC(gamma='scale', probability=True, class_weight='balanced', random_state=0)\n",
        "    clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    predictions = clf.predict(X_test_scaled)\n",
        "    probabilities = clf.predict_proba(X_test_scaled)\n",
        "\n",
        "    return predictions, probabilities\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"分類性能指標を計算\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    return {\n",
        "        'confusion_matrix': cm,\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
        "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
        "    }\n",
        "\n",
        "def analyze_improvement(baseline_results, proposed_results):\n",
        "    \"\"\"特徴量追加による改善を分析\"\"\"\n",
        "    baseline_correct = (baseline_results['y_true'] == baseline_results['y_pred'])\n",
        "    proposed_correct = (proposed_results['y_true'] == proposed_results['y_pred'])\n",
        "\n",
        "    newly_correct = (~baseline_correct) & proposed_correct\n",
        "    newly_incorrect = baseline_correct & (~proposed_correct)\n",
        "\n",
        "    return {\n",
        "        'newly_correct_count': newly_correct.sum(),\n",
        "        'newly_incorrect_count': newly_incorrect.sum(),\n",
        "        'net_improvement': newly_correct.sum() - newly_incorrect.sum(),\n",
        "        'newly_correct_indices': np.where(newly_correct)[0],\n",
        "        'newly_incorrect_indices': np.where(newly_incorrect)[0]\n",
        "    }\n",
        "\n",
        "def convert_numpy_to_list(obj):\n",
        "    \"\"\"NumPy配列を再帰的にリストに変換\"\"\"\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_to_list(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_to_list(item) for item in obj]\n",
        "    elif isinstance(obj, (np.integer, np.floating)):\n",
        "        return obj.item()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# ================ GPT-4o難易度特徴量の追加 ================\n",
        "print(\"\\nGPT-4o難易度特徴量を追加中...\")\n",
        "df_with_gpt = add_gpt_difficulty_features(df)\n",
        "\n",
        "# ================ メイン処理:文書別クロスバリデーション ================\n",
        "print(\"\\n文書別クロスバリデーション開始...\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "documents = sorted(df_with_gpt[\"id_document\"].unique())\n",
        "\n",
        "baseline_results = {'y_true': [], 'y_pred': [], 'y_prob': [], 'selected_features': []}\n",
        "proposed_results = {'y_true': [], 'y_pred': [], 'y_prob': [], 'selected_features': []}\n",
        "\n",
        "for test_doc in documents:\n",
        "    print(f\"\\n=== 文書 {test_doc} をテストデータとして使用 ===\")\n",
        "\n",
        "    test_data = df_with_gpt[df_with_gpt[\"id_document\"] == test_doc].copy()\n",
        "    train_data = df_with_gpt[df_with_gpt[\"id_document\"] != test_doc].copy()\n",
        "\n",
        "    print(f\"訓練データ: {len(train_data)} サンプル\")\n",
        "    print(f\"テストデータ: {len(test_data)} サンプル\")\n",
        "\n",
        "    # ================ Transformerモデルの訓練と特徴量抽出 ================\n",
        "    print(\"Transformerモデルを訓練中...\")\n",
        "\n",
        "    transformer_model = TransformerModel(\n",
        "        input_dim=1,\n",
        "        d_model=32,\n",
        "        nhead=2,\n",
        "        num_layers=1,\n",
        "        dim_feedforward=64,\n",
        "        dropout=0.2,\n",
        "        pooling='mean'\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "\n",
        "    # 訓練データ準備\n",
        "    train_lines = {}\n",
        "    train_labels = {}\n",
        "\n",
        "    print(f\"訓練データの準備中...\")\n",
        "\n",
        "    for _, row in train_data.iterrows():\n",
        "        line_id = str(row['id_line'] + 1)\n",
        "        x_seq = row['x_coordinates_dict'].get(line_id)\n",
        "        if x_seq and len(x_seq) > 0:\n",
        "            # 各line_idに対して最初に見つかったシーケンスのみを保持\n",
        "            if line_id not in train_lines:\n",
        "                train_lines[line_id] = x_seq\n",
        "                train_labels[line_id] = row['unknownWordLabel']\n",
        "\n",
        "    print(f\"  ユニークなline_id数: {len(train_lines)}\")\n",
        "\n",
        "    # Transformer訓練\n",
        "    X_transformer, Y_transformer = [], []\n",
        "    for lid, x_seq in train_lines.items():\n",
        "        # [[[x1], [x2], [x3], ...]] の形式に変換\n",
        "        x_tensor = torch.tensor([[[x] for x in x_seq]], dtype=torch.float32).to(device)\n",
        "        X_transformer.append(x_tensor)\n",
        "        Y_transformer.append(train_labels[lid])\n",
        "\n",
        "    # X_transformerが空の場合の処理\n",
        "    if len(X_transformer) == 0:\n",
        "        print(\"  ⚠️ 警告: 訓練データが見つかりませんでした。Transformer特徴量をスキップします。\")\n",
        "        train_transformer_features = {}\n",
        "        test_transformer_features = {}\n",
        "    else:\n",
        "        transformer_model.train()\n",
        "        epochs = 10\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for x_tensor, label in zip(X_transformer, Y_transformer):\n",
        "                optimizer.zero_grad()\n",
        "                out, _ = transformer_model(x_tensor)\n",
        "                loss = criterion(out, torch.tensor([label], dtype=torch.long).to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                print(f\"  Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(X_transformer):.4f}\")\n",
        "\n",
        "        # Transformer特徴量抽出\n",
        "        print(\"Transformer特徴量を抽出中...\")\n",
        "        train_transformer_features = extract_transformer_features(train_data, transformer_model, device)\n",
        "        test_transformer_features = extract_transformer_features(test_data, transformer_model, device)\n",
        "\n",
        "    # データにTransformer特徴量を追加\n",
        "    train_data_with_features = add_transformer_features_to_data(train_data, train_transformer_features)\n",
        "    test_data_with_features = add_transformer_features_to_data(test_data, test_transformer_features)\n",
        "\n",
        "    # ================ ベースライン手法（GPT-4o込み） ================\n",
        "    print(\"ベースライン手法（従来特徴量 + GPT-4o）を実行中...\")\n",
        "\n",
        "    # SHAP分析で30個に絞る\n",
        "    X_baseline_all = train_data[baseline_features].fillna(0).values\n",
        "    y_train = train_data[\"unknownWordLabel\"].values\n",
        "\n",
        "    baseline_shap_features = extract_shap_features(X_baseline_all, y_train, baseline_features, n_features=30)\n",
        "    X_baseline = train_data[baseline_shap_features].fillna(0).values\n",
        "\n",
        "    # SBFS特徴量選択\n",
        "    X_baseline_selected, baseline_selected_features = feature_selection_sbs(\n",
        "        X_baseline, y_train, baseline_shap_features)\n",
        "\n",
        "    X_test_baseline = test_data[baseline_selected_features].fillna(0).values\n",
        "    y_test = test_data[\"unknownWordLabel\"].values\n",
        "\n",
        "    baseline_pred, baseline_prob = train_and_predict(\n",
        "        X_baseline_selected, y_train, X_test_baseline, y_test)\n",
        "\n",
        "    baseline_results['y_true'].extend(y_test)\n",
        "    baseline_results['y_pred'].extend(baseline_pred)\n",
        "    baseline_results['y_prob'].extend(baseline_prob[:, 1])\n",
        "    baseline_results['selected_features'].append(baseline_selected_features)\n",
        "\n",
        "    # ================ 提案手法（ベースライン + Transformer特徴量） ================\n",
        "    print(\"提案手法（ベースライン + Transformer視線特徴量）を実行中...\")\n",
        "\n",
        "    # SHAP分析で30個に絞る\n",
        "    X_proposed_all = train_data_with_features[proposed_features].fillna(0).values\n",
        "\n",
        "    proposed_shap_features = extract_shap_features(X_proposed_all, y_train, proposed_features, n_features=30)\n",
        "    X_proposed = train_data_with_features[proposed_shap_features].fillna(0).values\n",
        "\n",
        "    # SBFS特徴量選択\n",
        "    X_proposed_selected, proposed_selected_features = feature_selection_sbs(\n",
        "        X_proposed, y_train, proposed_shap_features)\n",
        "\n",
        "    X_test_proposed = test_data_with_features[proposed_selected_features].fillna(0).values\n",
        "\n",
        "    proposed_pred, proposed_prob = train_and_predict(\n",
        "        X_proposed_selected, y_train, X_test_proposed, y_test)\n",
        "\n",
        "    proposed_results['y_true'].extend(y_test)\n",
        "    proposed_results['y_pred'].extend(proposed_pred)\n",
        "    proposed_results['y_prob'].extend(proposed_prob[:, 1])\n",
        "    proposed_results['selected_features'].append(proposed_selected_features)\n",
        "\n",
        "    baseline_metrics = calculate_metrics(y_test, baseline_pred)\n",
        "    proposed_metrics = calculate_metrics(y_test, proposed_pred)\n",
        "\n",
        "    print(f\"ベースライン (GPT-4o) - F1: {baseline_metrics['f1_score']:.3f}\")\n",
        "    print(f\"提案手法 (GPT-4o + Transformer) - F1: {proposed_metrics['f1_score']:.3f}\")\n",
        "\n",
        "# ================ 全体結果の分析 ================\n",
        "print(\"\\n=== 全体結果の分析 ===\")\n",
        "\n",
        "for key in ['y_true', 'y_pred', 'y_prob']:\n",
        "    baseline_results[key] = np.array(baseline_results[key])\n",
        "    proposed_results[key] = np.array(proposed_results[key])\n",
        "\n",
        "baseline_overall = calculate_metrics(baseline_results['y_true'], baseline_results['y_pred'])\n",
        "proposed_overall = calculate_metrics(proposed_results['y_true'], proposed_results['y_pred'])\n",
        "\n",
        "print(\"=== ベースライン手法（従来 + GPT-4o） ===\")\n",
        "print(f\"F1-score: {baseline_overall['f1_score']:.3f}\")\n",
        "print(f\"Accuracy: {baseline_overall['accuracy']:.3f}\")\n",
        "print(f\"Precision: {baseline_overall['precision']:.3f}\")\n",
        "print(f\"Recall: {baseline_overall['recall']:.3f}\")\n",
        "\n",
        "print(\"\\n=== 提案手法（ベースライン + Transformer視線情報） ===\")\n",
        "print(f\"F1-score: {proposed_overall['f1_score']:.3f}\")\n",
        "print(f\"Accuracy: {proposed_overall['accuracy']:.3f}\")\n",
        "print(f\"Precision: {proposed_overall['precision']:.3f}\")\n",
        "print(f\"Recall: {proposed_overall['recall']:.3f}\")\n",
        "\n",
        "print(f\"\\n=== Transformer視線特徴量の効果 ===\")\n",
        "print(f\"F1スコア改善: {proposed_overall['f1_score'] - baseline_overall['f1_score']:+.3f}\")\n",
        "\n",
        "improvement_analysis = analyze_improvement(baseline_results, proposed_results)\n",
        "\n",
        "print(f\"\\n新たに正しく分類された単語数: {improvement_analysis['newly_correct_count']}\")\n",
        "print(f\"新たに間違って分類された単語数: {improvement_analysis['newly_incorrect_count']}\")\n",
        "print(f\"純改善数: {improvement_analysis['net_improvement']}\")\n",
        "\n",
        "def get_difficulty_level(score):\n",
        "    \"\"\"難易度スコアを文字レベルに変換\"\"\"\n",
        "    if score <= 0.2:\n",
        "        return \"Very Easy (1-2)\"\n",
        "    elif score <= 0.4:\n",
        "        return \"Easy (3-4)\"\n",
        "    elif score <= 0.6:\n",
        "        return \"Moderate (5-6)\"\n",
        "    elif score <= 0.8:\n",
        "        return \"Difficult (7-8)\"\n",
        "    else:\n",
        "        return \"Very Difficult (9-10)\"\n",
        "\n",
        "# ================ 単語レベル詳細分析の保存 ================\n",
        "print(\"\\n単語レベルの詳細分析を保存中...\")\n",
        "\n",
        "word_level_analysis = []\n",
        "word_index = 0\n",
        "\n",
        "for test_doc in documents:\n",
        "    test_data = df_with_gpt[df_with_gpt[\"id_document\"] == test_doc].copy()\n",
        "\n",
        "    for _, row in test_data.iterrows():\n",
        "        word_info = {\n",
        "            'word_index': word_index,\n",
        "            'document_id': int(row['id_document']),\n",
        "            'line_id': int(row['id_line']),\n",
        "            'word_id': int(row['id_word']),\n",
        "            'word': row['word'],\n",
        "            'context': row['line'],  # Lineを文脈として使用\n",
        "            'gpt_difficulty': float(row['gpt_difficulty']),\n",
        "            'gpt_difficulty_integer': int(row['gpt_difficulty'] * 10),\n",
        "            'difficulty_level': get_difficulty_level(float(row['gpt_difficulty'])),\n",
        "            'true_label': int(baseline_results['y_true'][word_index]),\n",
        "            'baseline_prediction': int(baseline_results['y_pred'][word_index]),\n",
        "            'proposed_prediction': int(proposed_results['y_pred'][word_index]),\n",
        "            'baseline_probability': float(baseline_results['y_prob'][word_index]),\n",
        "            'proposed_probability': float(proposed_results['y_prob'][word_index]),\n",
        "            'baseline_correct': bool(baseline_results['y_true'][word_index] == baseline_results['y_pred'][word_index]),\n",
        "            'proposed_correct': bool(proposed_results['y_true'][word_index] == proposed_results['y_pred'][word_index]),\n",
        "        }\n",
        "\n",
        "        if not word_info['baseline_correct'] and word_info['proposed_correct']:\n",
        "            word_info['improvement_category'] = 'newly_correct'\n",
        "        elif word_info['baseline_correct'] and not word_info['proposed_correct']:\n",
        "            word_info['improvement_category'] = 'newly_incorrect'\n",
        "        elif word_info['baseline_correct'] and word_info['proposed_correct']:\n",
        "            word_info['improvement_category'] = 'both_correct'\n",
        "        else:\n",
        "            word_info['improvement_category'] = 'both_incorrect'\n",
        "\n",
        "        word_level_analysis.append(word_info)\n",
        "        word_index += 1\n",
        "\n",
        "# 新しく正解した単語のリスト作成（難易度順）\n",
        "newly_correct_words = [w for w in word_level_analysis if w['improvement_category'] == 'newly_correct']\n",
        "newly_correct_words.sort(key=lambda x: x['gpt_difficulty'], reverse=True)\n",
        "\n",
        "# 新しく誤判定した単語のリスト作成（難易度順）\n",
        "newly_incorrect_words = [w for w in word_level_analysis if w['improvement_category'] == 'newly_incorrect']\n",
        "newly_incorrect_words.sort(key=lambda x: x['gpt_difficulty'], reverse=True)\n",
        "\n",
        "# 新しく正解した単語の詳細ファイル保存\n",
        "with open(f'Output/GPT_Transformer/analysis/words/newly_correct_words_{experiment_number}.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"新しく正しく分類された単語一覧 (実験番号: {experiment_number})\\n\")\n",
        "    f.write(f\"総数: {len(newly_correct_words)} 単語\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    for i, word_info in enumerate(newly_correct_words, 1):\n",
        "        f.write(f\"[{i}] 単語: {word_info['word']}\\n\")\n",
        "        f.write(f\"    文脈: {word_info['context']}\\n\")\n",
        "        f.write(f\"    GPT-4o難易度スコア: {word_info['gpt_difficulty_integer']}/10 ({word_info['difficulty_level']})\\n\")\n",
        "        f.write(f\"    真のラベル: {'未知' if word_info['true_label']==1 else '既知'}\\n\")\n",
        "        f.write(f\"    ベースライン予測: {'未知' if word_info['baseline_prediction']==1 else '既知'}\\n\")\n",
        "        f.write(f\"    提案手法予測: {'未知' if word_info['proposed_prediction']==1 else '既知'}\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "# 新しく誤判定した単語の詳細ファイル保存\n",
        "with open(f'Output/GPT_Transformer/analysis/words/newly_incorrect_words_{experiment_number}.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"新しく誤って分類された単語一覧 (実験番号: {experiment_number})\\n\")\n",
        "    f.write(f\"総数: {len(newly_incorrect_words)} 単語\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    for i, word_info in enumerate(newly_incorrect_words, 1):\n",
        "        f.write(f\"[{i}] 単語: {word_info['word']}\\n\")\n",
        "        f.write(f\"    文脈: {word_info['context']}\\n\")\n",
        "        f.write(f\"    GPT-4o難易度スコア: {word_info['gpt_difficulty_integer']}/10 ({word_info['difficulty_level']})\\n\")\n",
        "        f.write(f\"    真のラベル: {'未知' if word_info['true_label']==1 else '既知'}\\n\")\n",
        "        f.write(f\"    ベースライン予測: {'未知' if word_info['baseline_prediction']==1 else '既知'}\\n\")\n",
        "        f.write(f\"    提案手法予測: {'未知' if word_info['proposed_prediction']==1 else '既知'}\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "print(f\"単語詳細ファイルを保存しました:\")\n",
        "print(f\"  - 新しく正解: Output/GPT_Transformer/analysis/words/newly_correct_words_{experiment_number}.txt\")\n",
        "print(f\"  - 新しく誤判定: Output/GPT_Transformer/analysis/words/newly_incorrect_words_{experiment_number}.txt\")\n",
        "\n",
        "# ================ 結果の可視化（個別画像保存） ================\n",
        "print(\"\\n結果を可視化中...\")\n",
        "\n",
        "# GPT-4o難易度の分布を確認\n",
        "gpt_scores = df_with_gpt['gpt_difficulty'].values\n",
        "unknown_scores = gpt_scores[df_with_gpt['unknownWordLabel'] == 1]\n",
        "known_scores = gpt_scores[df_with_gpt['unknownWordLabel'] == 0]\n",
        "\n",
        "# PR曲線の計算\n",
        "baseline_precision, baseline_recall, _ = precision_recall_curve(\n",
        "    baseline_results['y_true'], baseline_results['y_prob'])\n",
        "proposed_precision, proposed_recall, _ = precision_recall_curve(\n",
        "    proposed_results['y_true'], proposed_results['y_prob'])\n",
        "\n",
        "def interpolate_precision_recall(precision, recall):\n",
        "    precision_interp = []\n",
        "    for i in range(11):\n",
        "        recall_level = i / 10.0\n",
        "        max_precision = 0.0\n",
        "        for j, r in enumerate(recall):\n",
        "            if r >= recall_level and precision[j] > max_precision:\n",
        "                max_precision = precision[j]\n",
        "        precision_interp.append(max_precision)\n",
        "    return precision_interp, [i/10.0 for i in range(11)]\n",
        "\n",
        "baseline_prec_interp, recall_interp = interpolate_precision_recall(baseline_precision, baseline_recall)\n",
        "proposed_prec_interp, _ = interpolate_precision_recall(proposed_precision, proposed_recall)\n",
        "\n",
        "# 個別画像1: PR曲線\n",
        "fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
        "ax1.plot(recall_interp, proposed_prec_interp, 'b-o',\n",
        "         label=f'Proposed (AUC={auc(recall_interp, proposed_prec_interp):.3f})', markersize=4)\n",
        "ax1.plot(recall_interp, baseline_prec_interp, 'r-o',\n",
        "         label=f'Baseline (AUC={auc(recall_interp, baseline_prec_interp):.3f})', markersize=4)\n",
        "ax1.set_xlabel('Recall')\n",
        "ax1.set_ylabel('Precision')\n",
        "ax1.set_title(f'PR Curve (GPT-4o+Transformer) - Exp {experiment_number}')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'Output/GPT_Transformer/aupr/individual/PR_curve_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# 個別画像2: F1スコア比較\n",
        "fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
        "ax2.bar(['Baseline\\n(従来+GPT-4o)', 'Proposed\\n(+Transformer)'],\n",
        "        [baseline_overall['f1_score'], proposed_overall['f1_score']],\n",
        "        color=['red', 'blue'], alpha=0.7)\n",
        "ax2.set_ylabel('F1-Score')\n",
        "ax2.set_title(f'F1 Comparison - Exp {experiment_number}')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "# F1スコア改善を表示\n",
        "improvement = proposed_overall['f1_score'] - baseline_overall['f1_score']\n",
        "if improvement > 0:\n",
        "    ax2.text(1, proposed_overall['f1_score'], f'+{improvement:.3f}',\n",
        "             ha='center', va='bottom', fontsize=10, color='blue')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'Output/GPT_Transformer/aupr/individual/F1_comparison_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# 個別画像3: 改善分析\n",
        "fig3, ax3 = plt.subplots(figsize=(8, 6))\n",
        "categories = ['Newly Correct', 'Newly Incorrect', 'Net Improvement']\n",
        "values = [improvement_analysis['newly_correct_count'],\n",
        "          improvement_analysis['newly_incorrect_count'],\n",
        "          improvement_analysis['net_improvement']]\n",
        "colors = ['green', 'red', 'blue']\n",
        "\n",
        "bars = ax3.bar(categories, values, color=colors, alpha=0.7)\n",
        "ax3.set_ylabel('Number of Words')\n",
        "ax3.set_title(f'Impact (GPT-4o+Transformer) - Exp {experiment_number}')\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, value in zip(bars, values):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "             f'{value}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'Output/GPT_Transformer/aupr/individual/improvement_analysis_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# 個別画像4: GPT-4o難易度分布比較\n",
        "fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
        "ax4.hist(known_scores, bins=10, alpha=0.7,\n",
        "         label=f'Known Words (n={len(known_scores)})', color='blue', density=True)\n",
        "ax4.hist(unknown_scores, bins=10, alpha=0.7,\n",
        "         label=f'Unknown Words (n={len(unknown_scores)})', color='red', density=True)\n",
        "ax4.set_xlabel('GPT-4o Difficulty Score')\n",
        "ax4.set_ylabel('Density')\n",
        "ax4.set_title(f'Difficulty Distribution - Exp {experiment_number}')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'Output/GPT_Transformer/aupr/individual/difficulty_distribution_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"個別画像を保存しました:\")\n",
        "print(f\"  - Output/GPT_Transformer/aupr/individual/PR_curve_{experiment_number}.png\")\n",
        "print(f\"  - Output/GPT_Transformer/aupr/individual/F1_comparison_{experiment_number}.png\")\n",
        "print(f\"  - Output/GPT_Transformer/aupr/individual/improvement_analysis_{experiment_number}.png\")\n",
        "print(f\"  - Output/GPT_Transformer/aupr/individual/difficulty_distribution_{experiment_number}.png\")\n",
        "\n",
        "# ================ 結果の保存 ================\n",
        "print(\"\\n結果を保存中...\")\n",
        "\n",
        "with open(f'Output/GPT_Transformer/selected_features/baseline_features_{experiment_number}.pkl', 'wb') as f:\n",
        "    pickle.dump(baseline_results['selected_features'], f)\n",
        "\n",
        "with open(f'Output/GPT_Transformer/selected_features/proposed_features_{experiment_number}.pkl', 'wb') as f:\n",
        "    pickle.dump(proposed_results['selected_features'], f)\n",
        "\n",
        "analysis_results = {\n",
        "    'experiment_number': experiment_number,\n",
        "    'baseline_results': convert_numpy_to_list(baseline_overall),\n",
        "    'proposed_results': convert_numpy_to_list(proposed_overall),\n",
        "    'improvement_analysis': convert_numpy_to_list(improvement_analysis),\n",
        "    'word_level_analysis': word_level_analysis\n",
        "}\n",
        "\n",
        "with open(f'Output/GPT_Transformer/analysis/complete_analysis_{experiment_number}.json', 'w') as f:\n",
        "    json.dump(analysis_results, f, indent=2)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"処理完了！ 総実行時間: {elapsed_time/60:.2f} 分\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n結果保存先: Output/GPT_Transformer/\")\n",
        "print(f\"  - aupr/individual/ : 個別画像\")\n",
        "print(f\"  - analysis/words/ : 単語詳細\")\n",
        "print(f\"  - analysis/ : 分析結果JSON\")\n",
        "print(f\"  - selected_features/ : 選択特徴量\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "9ACBItt-Y7Ml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf98251-9f83-4ea4-e6d0-2ce89efaa962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "実験番号 c04 のGPT-4o + Transformer難易度評価分析を開始します...\n",
            "✅ OpenAI APIクライアントが正常に初期化されました。\n",
            "使用モデル: gpt-4o + Transformer\n",
            "処理開始時刻: 2025-12-24 15:47:51\n",
            "データを読み込んでいます...\n",
            "データ形状: (2452, 31)\n",
            "未知単語ラベルの分布: Counter({0: 2335, 1: 117})\n",
            "\n",
            "lineカラムから文脈データを取得中...\n",
            "文脈取得完了: 2452 単語\n",
            "\n",
            "特徴量構成:\n",
            "  従来特徴量: 22個\n",
            "  ベースライン (従来 + GPT-4o): 23個\n",
            "  提案手法 (ベースライン + Transformer): 55個\n",
            "\n",
            "GPT-4o難易度特徴量を追加中...\n",
            "gpt-4oによる難易度評価を実行中（新規評価）...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2452/2452 [17:03<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "文書別クロスバリデーション開始...\n",
            "\n",
            "=== 文書 0 をテストデータとして使用 ===\n",
            "訓練データ: 2185 サンプル\n",
            "テストデータ: 267 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 81\n",
            "  Epoch 2/10, Loss: 0.4325\n",
            "  Epoch 4/10, Loss: 0.4260\n",
            "  Epoch 6/10, Loss: 0.4194\n",
            "  Epoch 8/10, Loss: 0.4157\n",
            "  Epoch 10/10, Loss: 0.4069\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 8\n",
            "選択された特徴量: ['freq', 'gpt_difficulty', 'Num_Monosyllable', 'ContentWord', 'Num_ContentWords', 'Rate_FunctionWords', 'freq_Max', 'Num_Polysyllable']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 9\n",
            "選択された特徴量: ['freq', 'gpt_difficulty', 'transformer25', 'Num_Monosyllable', 'transformer26', 'transformer2', 'ContentWord', 'transformer15', 'Rate_7Characters']\n",
            "ベースライン (GPT-4o) - F1: 0.161\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.145\n",
            "\n",
            "=== 文書 1 をテストデータとして使用 ===\n",
            "訓練データ: 2189 サンプル\n",
            "テストデータ: 263 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 81\n",
            "  Epoch 2/10, Loss: 0.1712\n",
            "  Epoch 4/10, Loss: 0.1684\n",
            "  Epoch 6/10, Loss: 0.1667\n",
            "  Epoch 8/10, Loss: 0.1656\n",
            "  Epoch 10/10, Loss: 0.1643\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 11\n",
            "選択された特徴量: ['freq', 'gpt_difficulty', 'freq_Ave', 'ContentWord', 'Num_Monosyllable', 'Rate_FunctionWords', 'Num_Polysyllable', 'freq_Max', 'Num_FunctionWords', 'ReadBack', 'Rate_7Characters']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 20\n",
            "選択された特徴量: ['transformer21', 'freq', 'transformer0', 'transformer9', 'transformer29', 'transformer23', 'transformer7', 'transformer17', 'transformer26', 'transformer30', 'transformer10', 'transformer20', 'transformer15', 'transformer11', 'transformer1', 'transformer5', 'Num_ContentWords', 'transformer24', 'transformer12', 'transformer14']\n",
            "ベースライン (GPT-4o) - F1: 0.431\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.491\n",
            "\n",
            "=== 文書 2 をテストデータとして使用 ===\n",
            "訓練データ: 2144 サンプル\n",
            "テストデータ: 308 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 81\n",
            "  Epoch 2/10, Loss: 0.1239\n",
            "  Epoch 4/10, Loss: 0.1230\n",
            "  Epoch 6/10, Loss: 0.1220\n",
            "  Epoch 8/10, Loss: 0.1235\n",
            "  Epoch 10/10, Loss: 0.1198\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 5\n",
            "選択された特徴量: ['freq', 'Rate_7Characters', 'ContentWord', 'Num_Words', 'Num_Polysyllable']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 20\n",
            "選択された特徴量: ['transformer20', 'Num_Monosyllable', 'transformer11', 'transformer6', 'gpt_difficulty', 'freq', 'transformer9', 'transformer26', 'Num_7Characters', 'transformer7', 'transformer18', 'Length_Word_Ave', 'Rate_7Characters', 'ContentWord', 'transformer3', 'transformer22', 'transformer21', 'Num_Words', 'transformer30', 'transformer24']\n",
            "ベースライン (GPT-4o) - F1: 0.351\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.405\n",
            "\n",
            "=== 文書 3 をテストデータとして使用 ===\n",
            "訓練データ: 2174 サンプル\n",
            "テストデータ: 278 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 81\n",
            "  Epoch 2/10, Loss: 0.1690\n",
            "  Epoch 4/10, Loss: 0.1669\n",
            "  Epoch 6/10, Loss: 0.1655\n",
            "  Epoch 8/10, Loss: 0.1643\n",
            "  Epoch 10/10, Loss: 0.1628\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 9\n",
            "選択された特徴量: ['gpt_difficulty', 'freq', 'ContentWord', 'syllables', 'Rate_7Characters', 'ReadBack', 'Num_7Characters', 'seven_character', 'Num_Polysyllable']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 19\n",
            "選択された特徴量: ['freq', 'transformer24', 'transformer27', 'transformer0', 'transformer10', 'transformer6', 'ContentWord', 'transformer13', 'transformer25', 'transformer31', 'transformer12', 'transformer8', 'transformer29', 'transformer23', 'transformer14', 'transformer1', 'transformer11', 'transformer3', 'transformer19']\n",
            "ベースライン (GPT-4o) - F1: 0.333\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.260\n",
            "\n",
            "=== 文書 4 をテストデータとして使用 ===\n",
            "訓練データ: 2222 サンプル\n",
            "テストデータ: 230 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 81\n",
            "  Epoch 2/10, Loss: 0.1739\n",
            "  Epoch 4/10, Loss: 0.1701\n",
            "  Epoch 6/10, Loss: 0.1664\n",
            "  Epoch 8/10, Loss: 0.1625\n",
            "  Epoch 10/10, Loss: 0.1606\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 4\n",
            "選択された特徴量: ['freq', 'ContentWord', 'Num_Words', 'seven_character']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 5\n",
            "選択された特徴量: ['freq', 'transformer11', 'transformer30', 'Num_ContentWords', 'Num_Words']\n",
            "ベースライン (GPT-4o) - F1: 0.306\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.308\n",
            "\n",
            "=== 文書 5 をテストデータとして使用 ===\n",
            "訓練データ: 2188 サンプル\n",
            "テストデータ: 264 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 81\n",
            "  Epoch 2/10, Loss: 0.1725\n",
            "  Epoch 4/10, Loss: 0.1689\n",
            "  Epoch 6/10, Loss: 0.1663\n",
            "  Epoch 8/10, Loss: 0.1659\n",
            "  Epoch 10/10, Loss: 0.1646\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 8\n",
            "選択された特徴量: ['freq', 'gpt_difficulty', 'ContentWord', 'seven_character', 'Num_Polysyllable', 'syllables', 'ReadBack', 'Rate_7Characters']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 13\n",
            "選択された特徴量: ['freq', 'transformer1', 'transformer27', 'freq_Ave', 'transformer15', 'transformer12', 'transformer8', 'transformer6', 'transformer10', 'transformer22', 'Rate_FunctionWords', 'Rate_ContentWords', 'ContentWord']\n",
            "ベースライン (GPT-4o) - F1: 0.390\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.314\n",
            "\n",
            "=== 文書 6 をテストデータとして使用 ===\n",
            "訓練データ: 2177 サンプル\n",
            "テストデータ: 275 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 81\n",
            "  Epoch 2/10, Loss: 0.1734\n",
            "  Epoch 4/10, Loss: 0.1702\n",
            "  Epoch 6/10, Loss: 0.1679\n",
            "  Epoch 8/10, Loss: 0.1677\n",
            "  Epoch 10/10, Loss: 0.1647\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 9\n",
            "選択された特徴量: ['gpt_difficulty', 'freq', 'Num_Monosyllable', 'ContentWord', 'freq_Ave', 'Num_7Characters', 'ReadBack', 'Num_Polysyllable', 'Num_FunctionWords']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 19\n",
            "選択された特徴量: ['transformer26', 'gpt_difficulty', 'freq', 'transformer15', 'transformer19', 'Num_Monosyllable', 'transformer22', 'transformer21', 'transformer12', 'transformer9', 'transformer3', 'Num_7Characters', 'transformer28', 'ContentWord', 'freq_Ave', 'transformer18', 'transformer13', 'transformer6', 'transformer24']\n",
            "ベースライン (GPT-4o) - F1: 0.353\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.263\n",
            "\n",
            "=== 文書 7 をテストデータとして使用 ===\n",
            "訓練データ: 2180 サンプル\n",
            "テストデータ: 272 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 81\n",
            "  Epoch 2/10, Loss: 0.1689\n",
            "  Epoch 4/10, Loss: 0.1670\n",
            "  Epoch 6/10, Loss: 0.1665\n",
            "  Epoch 8/10, Loss: 0.1653\n",
            "  Epoch 10/10, Loss: 0.1647\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 11\n",
            "選択された特徴量: ['freq', 'gpt_difficulty', 'ContentWord', 'Num_ContentWords', 'ReadBack', 'Rate_FunctionWords', 'Rate_ContentWords', 'freq_Min', 'Rate_7Characters', 'seven_character', 'Num_Polysyllable']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 15\n",
            "選択された特徴量: ['freq', 'transformer14', 'transformer30', 'transformer18', 'transformer13', 'transformer9', 'transformer4', 'transformer8', 'freq_Ave', 'transformer12', 'transformer16', 'ContentWord', 'transformer6', 'Num_7Characters', 'transformer19']\n",
            "ベースライン (GPT-4o) - F1: 0.067\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.114\n",
            "\n",
            "=== 文書 8 をテストデータとして使用 ===\n",
            "訓練データ: 2157 サンプル\n",
            "テストデータ: 295 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 77\n",
            "  Epoch 2/10, Loss: 0.1250\n",
            "  Epoch 4/10, Loss: 0.1233\n",
            "  Epoch 6/10, Loss: 0.1215\n",
            "  Epoch 8/10, Loss: 0.1184\n",
            "  Epoch 10/10, Loss: 0.1121\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 6\n",
            "選択された特徴量: ['freq', 'Num_Monosyllable', 'ContentWord', 'Num_Words', 'ReadBack', 'Num_7Characters']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 19\n",
            "選択された特徴量: ['transformer12', 'transformer29', 'freq', 'transformer28', 'transformer21', 'transformer19', 'transformer31', 'transformer14', 'transformer13', 'freq_Ave', 'transformer18', 'ContentWord', 'transformer30', 'transformer22', 'transformer11', 'transformer2', 'transformer25', 'transformer16', 'transformer15']\n",
            "ベースライン (GPT-4o) - F1: 0.413\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.424\n",
            "\n",
            "=== 全体結果の分析 ===\n",
            "=== ベースライン手法（従来 + GPT-4o） ===\n",
            "F1-score: 0.333\n",
            "Accuracy: 0.840\n",
            "Precision: 0.208\n",
            "Recall: 0.838\n",
            "\n",
            "=== 提案手法（ベースライン + Transformer視線情報） ===\n",
            "F1-score: 0.316\n",
            "Accuracy: 0.839\n",
            "Precision: 0.198\n",
            "Recall: 0.778\n",
            "\n",
            "=== Transformer視線特徴量の効果 ===\n",
            "F1スコア改善: -0.017\n",
            "\n",
            "新たに正しく分類された単語数: 104\n",
            "新たに間違って分類された単語数: 105\n",
            "純改善数: -1\n",
            "\n",
            "単語レベルの詳細分析を保存中...\n",
            "単語詳細ファイルを保存しました:\n",
            "  - 新しく正解: Output/GPT_Transformer/analysis/words/newly_correct_words_c04.txt\n",
            "  - 新しく誤判定: Output/GPT_Transformer/analysis/words/newly_incorrect_words_c04.txt\n",
            "\n",
            "結果を可視化中...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-655409961.py:659: UserWarning: Glyph 24467 (\\N{CJK UNIFIED IDEOGRAPH-5F93}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-655409961.py:659: UserWarning: Glyph 26469 (\\N{CJK UNIFIED IDEOGRAPH-6765}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-655409961.py:660: UserWarning: Glyph 24467 (\\N{CJK UNIFIED IDEOGRAPH-5F93}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(f'Output/GPT_Transformer/aupr/individual/F1_comparison_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
            "/tmp/ipython-input-655409961.py:660: UserWarning: Glyph 26469 (\\N{CJK UNIFIED IDEOGRAPH-6765}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(f'Output/GPT_Transformer/aupr/individual/F1_comparison_{experiment_number}.png', dpi=300, bbox_inches='tight')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "個別画像を保存しました:\n",
            "  - Output/GPT_Transformer/aupr/individual/PR_curve_c04.png\n",
            "  - Output/GPT_Transformer/aupr/individual/F1_comparison_c04.png\n",
            "  - Output/GPT_Transformer/aupr/individual/improvement_analysis_c04.png\n",
            "  - Output/GPT_Transformer/aupr/individual/difficulty_distribution_c04.png\n",
            "\n",
            "結果を保存中...\n",
            "\n",
            "============================================================\n",
            "処理完了！ 総実行時間: 203.45 分\n",
            "============================================================\n",
            "\n",
            "結果保存先: Output/GPT_Transformer/\n",
            "  - aupr/individual/ : 個別画像\n",
            "  - analysis/words/ : 単語詳細\n",
            "  - analysis/ : 分析結果JSON\n",
            "  - selected_features/ : 選択特徴量\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================ 設定 ================\n",
        "experiment_number = 'c07'  # 実験番号（要変更）\n",
        "print(f\"実験番号 {experiment_number} のGPT-4o + Transformer難易度評価分析を開始します...\")\n",
        "\n",
        "# 出力ディレクトリの作成\n",
        "os.makedirs('Output/GPT_Transformer/aupr', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/aupr/individual', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/selected_features', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/analysis', exist_ok=True)\n",
        "os.makedirs('Output/GPT_Transformer/analysis/words', exist_ok=True)\n",
        "\n",
        "# OpenAI APIクライアントの初期化\n",
        "try:\n",
        "    client = OpenAI(api_key=userdata.get('API_KEY'))\n",
        "    if client is None:\n",
        "        print(\"⚠️ OpenAI APIキーが未設定です。\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"✅ OpenAI APIクライアントが正常に初期化されました。\")\n",
        "        print(\"使用モデル: gpt-4o + Transformer\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ OpenAI初期化エラー: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 実行時間計測開始\n",
        "start_time = time.time()\n",
        "start_datetime = datetime.now()\n",
        "print(f\"処理開始時刻: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# ================ データの読み込み ================\n",
        "print(\"データを読み込んでいます...\")\n",
        "# 予備実験のLSTM特徴量ファイルから読み込み\n",
        "df = pd.read_json(f'Output/LSTM/features/{experiment_number}-features.json')\n",
        "df = df.dropna()\n",
        "\n",
        "print(f\"データ形状: {df.shape}\")\n",
        "print(f\"未知単語ラベルの分布: {Counter(df['unknownWordLabel'])}\")\n",
        "\n",
        "# ================ lineカラムから文脈を取得 ================\n",
        "print(\"\\nlineカラムから文脈データを取得中...\")\n",
        "\n",
        "# lineカラムが既に存在することを確認\n",
        "if 'line' not in df.columns:\n",
        "    print(\"❌ エラー: 'line'カラムがデータフレームに存在しません。\")\n",
        "    exit()\n",
        "\n",
        "# lineカラムをsentenceとして使用\n",
        "df['sentence'] = df['line']\n",
        "print(f\"文脈取得完了: {len(df)} 単語\")\n",
        "\n",
        "# ================ GPT-4o難易度評価関数 ================\n",
        "def get_gpt_difficulty_score(word, context_sentence, max_retries=3):\n",
        "    \"\"\"gpt-4oを使用して単語の文脈的難易度を1-10の整数値で評価\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an expert English language difficulty assessor for second language learners.\n",
        "\n",
        "Please evaluate the difficulty of the word \"{word}\" in this specific context for an intermediate English learner:\n",
        "\n",
        "Context: \"{context_sentence}\"\n",
        "\n",
        "Consider these factors:\n",
        "- Semantic complexity and abstractness in this context\n",
        "- Collocational patterns and usage constraints\n",
        "\n",
        "Provide a precise integer score between 1 and 10:\n",
        "- 1-2: Very easy (basic vocabulary, high frequency)\n",
        "- 3-4: Easy (common words, straightforward usage)\n",
        "- 5-6: Moderate (intermediate vocabulary, some complexity)\n",
        "- 7-8: Difficult (advanced vocabulary, complex usage)\n",
        "- 9-10: Very difficult (rare, highly technical, or complex)\n",
        "\n",
        "Return ONLY the integer number (e.g., 2, 6, 9). Do not provide explanations.\n",
        "\n",
        "Score:\n",
        "\"\"\"\n",
        "\n",
        "    for retry in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a precise language difficulty assessor. Respond ONLY with an integer number between 1 and 10.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=10,\n",
        "                temperature=0.0,\n",
        "            )\n",
        "\n",
        "            score_text = response.choices[0].message.content.strip()\n",
        "            score_match = re.search(r'\\d+', score_text)\n",
        "\n",
        "            if score_match:\n",
        "                score_int = int(score_match.group())\n",
        "                score_int = max(1, min(10, score_int))\n",
        "                normalized_score = score_int / 10.0\n",
        "                return normalized_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"GPT-4o API エラー (試行 {retry + 1}): {str(e)}\")\n",
        "            if retry < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "\n",
        "    raise Exception(f\"GPT-4o評価に失敗: {word}\")\n",
        "\n",
        "def add_gpt_difficulty_features(data):\n",
        "    \"\"\"データフレームにGPT-4o難易度評価特徴量を追加\"\"\"\n",
        "    gpt_difficulties = []\n",
        "\n",
        "    print(\"gpt-4oによる難易度評価を実行中（新規評価）...\")\n",
        "    for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
        "        word = row['word']\n",
        "        sentence = row['sentence']\n",
        "\n",
        "        try:\n",
        "            difficulty = get_gpt_difficulty_score(word, sentence)\n",
        "        except Exception as e:\n",
        "            print(f\"評価失敗: {word} - {str(e)}\")\n",
        "            difficulty = 0.5\n",
        "\n",
        "        gpt_difficulties.append(difficulty)\n",
        "\n",
        "    data_with_gpt = data.copy()\n",
        "    data_with_gpt['gpt_difficulty'] = gpt_difficulties\n",
        "\n",
        "    return data_with_gpt\n",
        "\n",
        "# ================ Transformerモデル定義 ================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim=1, d_model=32, nhead=2, num_layers=1,\n",
        "                 dim_feedforward=64, dropout=0.2, pooling='mean'):\n",
        "        super().__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, 2)\n",
        "        self.pooling = pooling\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_dim)\n",
        "        x = self.input_projection(x)  # (batch, seq_len, d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pooling == 'mean':\n",
        "            x_pooled = x.mean(dim=1)  # (batch, d_model)\n",
        "        elif self.pooling == 'max':\n",
        "            x_pooled = x.max(dim=1)[0]\n",
        "        else:  # last\n",
        "            x_pooled = x[:, -1, :]\n",
        "\n",
        "        out = self.fc(x_pooled)\n",
        "        return out, x_pooled\n",
        "\n",
        "# ================ 特徴量の定義 ================\n",
        "baseline_features_original = [\n",
        "    'length', 'freq', 'seven_character', 'ContentWord', 'syllables',\n",
        "    'Num_Words', 'Length_Word_Ave', 'freq_Min', 'freq_Max', 'freq_Ave',\n",
        "    'Num_7Characters', 'Rate_7Characters',\n",
        "    'Num_ContentWords', 'Rate_ContentWords',\n",
        "    'Num_FunctionWords', 'Rate_FunctionWords',\n",
        "    'Num_Monosyllable', 'Num_Polysyllable',\n",
        "    'Flesch_Reading_Ease', 'ARI', 'Readtime', 'ReadBack'\n",
        "]\n",
        "\n",
        "# ベースライン: 従来特徴量 + GPT-4o難易度\n",
        "gpt_features = ['gpt_difficulty']\n",
        "baseline_features = baseline_features_original + gpt_features\n",
        "\n",
        "# 提案手法: ベースライン + Transformer視線特徴量\n",
        "transformer_features = [f'transformer{i}' for i in range(32)]\n",
        "proposed_features = baseline_features + transformer_features\n",
        "\n",
        "print(\"\\n特徴量構成:\")\n",
        "print(f\"  従来特徴量: {len(baseline_features_original)}個\")\n",
        "print(f\"  ベースライン (従来 + GPT-4o): {len(baseline_features)}個\")\n",
        "print(f\"  提案手法 (ベースライン + Transformer): {len(proposed_features)}個\")\n",
        "\n",
        "# ================ ユーティリティ関数 ================\n",
        "def extract_shap_features(X, y, feature_names, n_features=30):\n",
        "    \"\"\"SHAP値を用いて重要な特徴量を抽出\"\"\"\n",
        "    print(f\"SHAP分析により上位{n_features}特徴量を選択中...\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    model = LogisticRegression(class_weight=\"balanced\", max_iter=1000, random_state=0)\n",
        "    model.fit(X_scaled, y)\n",
        "\n",
        "    explainer = shap.Explainer(model, X_scaled)\n",
        "    shap_values = explainer(X_scaled)\n",
        "\n",
        "    mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "    top_idx = np.argsort(mean_abs_shap)[::-1][:n_features]\n",
        "    selected_features = [feature_names[i] for i in top_idx]\n",
        "\n",
        "    print(f\"SHAP選択完了: {len(selected_features)}個の特徴量\")\n",
        "    return selected_features\n",
        "def extract_transformer_features(data, transformer_model, device):\n",
        "    \"\"\"Transformerモデルから特徴量を抽出\"\"\"\n",
        "    transformer_model.eval()\n",
        "    line_features = {}\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        line_id = str(row['id_line'] + 1)\n",
        "        x_seq = row['x_coordinates_dict'].get(line_id)\n",
        "\n",
        "        if x_seq and len(x_seq) > 0:\n",
        "            # LSTMと同じ形式: [[[x_seq]]]\n",
        "            x_tensor = torch.tensor([[[x] for x in x_seq]], dtype=torch.float32).to(device)\n",
        "            with torch.no_grad():\n",
        "                _, features = transformer_model(x_tensor)\n",
        "            line_features[row.name] = features.squeeze(0).cpu().numpy()\n",
        "\n",
        "    return line_features\n",
        "\n",
        "def add_transformer_features_to_data(data, line_features, n_features=32):\n",
        "    \"\"\"データフレームにTransformer特徴量を追加\"\"\"\n",
        "    for i in range(n_features):\n",
        "        data[f'transformer{i}'] = [\n",
        "            line_features[idx][i] if idx in line_features else 0.0\n",
        "            for idx in data.index\n",
        "        ]\n",
        "    return data\n",
        "\n",
        "def feature_selection_sbs(X, y, feature_names):\n",
        "    \"\"\"Sequential Backward Selection (SBS)による特徴量選択\"\"\"\n",
        "    print(\"Sequential Backward Selection (SBS)実行中...\")\n",
        "\n",
        "    clf = SVC(gamma='scale', probability=True, class_weight='balanced', random_state=0)\n",
        "    selector = sfs(clf,\n",
        "                   k_features=(1, len(feature_names)),\n",
        "                   forward=False,\n",
        "                   floating=False,  # Floatingを無効化してSBSに\n",
        "                   scoring='f1',\n",
        "                   cv=list(StratifiedKFold(n_splits=5, shuffle=True, random_state=0).split(X, y)),\n",
        "                   n_jobs=-1)\n",
        "\n",
        "    X_df = pd.DataFrame(X, columns=feature_names)\n",
        "    selector = selector.fit(X_df, y)\n",
        "\n",
        "    selected_features = X_df.columns[list(selector.k_feature_idx_)].tolist()\n",
        "    X_selected = X_df[selected_features].values\n",
        "\n",
        "    print(f\"選択された特徴量数: {len(selected_features)}\")\n",
        "    print(f\"選択された特徴量: {selected_features}\")\n",
        "\n",
        "    return X_selected, selected_features\n",
        "\n",
        "def train_and_predict(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"SVMモデルの訓練と予測\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    clf = SVC(gamma='scale', probability=True, class_weight='balanced', random_state=0)\n",
        "    clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "    predictions = clf.predict(X_test_scaled)\n",
        "    probabilities = clf.predict_proba(X_test_scaled)\n",
        "\n",
        "    return predictions, probabilities\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"分類性能指標を計算\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    return {\n",
        "        'confusion_matrix': cm,\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
        "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
        "    }\n",
        "\n",
        "def analyze_improvement(baseline_results, proposed_results):\n",
        "    \"\"\"特徴量追加による改善を分析\"\"\"\n",
        "    baseline_correct = (baseline_results['y_true'] == baseline_results['y_pred'])\n",
        "    proposed_correct = (proposed_results['y_true'] == proposed_results['y_pred'])\n",
        "\n",
        "    newly_correct = (~baseline_correct) & proposed_correct\n",
        "    newly_incorrect = baseline_correct & (~proposed_correct)\n",
        "\n",
        "    return {\n",
        "        'newly_correct_count': newly_correct.sum(),\n",
        "        'newly_incorrect_count': newly_incorrect.sum(),\n",
        "        'net_improvement': newly_correct.sum() - newly_incorrect.sum(),\n",
        "        'newly_correct_indices': np.where(newly_correct)[0],\n",
        "        'newly_incorrect_indices': np.where(newly_incorrect)[0]\n",
        "    }\n",
        "\n",
        "def convert_numpy_to_list(obj):\n",
        "    \"\"\"NumPy配列を再帰的にリストに変換\"\"\"\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_to_list(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_to_list(item) for item in obj]\n",
        "    elif isinstance(obj, (np.integer, np.floating)):\n",
        "        return obj.item()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# ================ GPT-4o難易度特徴量の追加 ================\n",
        "print(\"\\nGPT-4o難易度特徴量を追加中...\")\n",
        "df_with_gpt = add_gpt_difficulty_features(df)\n",
        "\n",
        "# ================ メイン処理:文書別クロスバリデーション ================\n",
        "print(\"\\n文書別クロスバリデーション開始...\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "documents = sorted(df_with_gpt[\"id_document\"].unique())\n",
        "\n",
        "baseline_results = {'y_true': [], 'y_pred': [], 'y_prob': [], 'selected_features': []}\n",
        "proposed_results = {'y_true': [], 'y_pred': [], 'y_prob': [], 'selected_features': []}\n",
        "\n",
        "for test_doc in documents:\n",
        "    print(f\"\\n=== 文書 {test_doc} をテストデータとして使用 ===\")\n",
        "\n",
        "    test_data = df_with_gpt[df_with_gpt[\"id_document\"] == test_doc].copy()\n",
        "    train_data = df_with_gpt[df_with_gpt[\"id_document\"] != test_doc].copy()\n",
        "\n",
        "    print(f\"訓練データ: {len(train_data)} サンプル\")\n",
        "    print(f\"テストデータ: {len(test_data)} サンプル\")\n",
        "\n",
        "    # ================ Transformerモデルの訓練と特徴量抽出 ================\n",
        "    print(\"Transformerモデルを訓練中...\")\n",
        "\n",
        "    transformer_model = TransformerModel(\n",
        "        input_dim=1,\n",
        "        d_model=32,\n",
        "        nhead=2,\n",
        "        num_layers=1,\n",
        "        dim_feedforward=64,\n",
        "        dropout=0.2,\n",
        "        pooling='mean'\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "\n",
        "    # 訓練データ準備\n",
        "    train_lines = {}\n",
        "    train_labels = {}\n",
        "\n",
        "    print(f\"訓練データの準備中...\")\n",
        "\n",
        "    for _, row in train_data.iterrows():\n",
        "        line_id = str(row['id_line'] + 1)\n",
        "        x_seq = row['x_coordinates_dict'].get(line_id)\n",
        "        if x_seq and len(x_seq) > 0:\n",
        "            # 各line_idに対して最初に見つかったシーケンスのみを保持\n",
        "            if line_id not in train_lines:\n",
        "                train_lines[line_id] = x_seq\n",
        "                train_labels[line_id] = row['unknownWordLabel']\n",
        "\n",
        "    print(f\"  ユニークなline_id数: {len(train_lines)}\")\n",
        "\n",
        "    # Transformer訓練\n",
        "    X_transformer, Y_transformer = [], []\n",
        "    for lid, x_seq in train_lines.items():\n",
        "        # [[[x1], [x2], [x3], ...]] の形式に変換\n",
        "        x_tensor = torch.tensor([[[x] for x in x_seq]], dtype=torch.float32).to(device)\n",
        "        X_transformer.append(x_tensor)\n",
        "        Y_transformer.append(train_labels[lid])\n",
        "\n",
        "    # X_transformerが空の場合の処理\n",
        "    if len(X_transformer) == 0:\n",
        "        print(\"  ⚠️ 警告: 訓練データが見つかりませんでした。Transformer特徴量をスキップします。\")\n",
        "        train_transformer_features = {}\n",
        "        test_transformer_features = {}\n",
        "    else:\n",
        "        transformer_model.train()\n",
        "        epochs = 10\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for x_tensor, label in zip(X_transformer, Y_transformer):\n",
        "                optimizer.zero_grad()\n",
        "                out, _ = transformer_model(x_tensor)\n",
        "                loss = criterion(out, torch.tensor([label], dtype=torch.long).to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                print(f\"  Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(X_transformer):.4f}\")\n",
        "\n",
        "        # Transformer特徴量抽出\n",
        "        print(\"Transformer特徴量を抽出中...\")\n",
        "        train_transformer_features = extract_transformer_features(train_data, transformer_model, device)\n",
        "        test_transformer_features = extract_transformer_features(test_data, transformer_model, device)\n",
        "\n",
        "    # データにTransformer特徴量を追加\n",
        "    train_data_with_features = add_transformer_features_to_data(train_data, train_transformer_features)\n",
        "    test_data_with_features = add_transformer_features_to_data(test_data, test_transformer_features)\n",
        "\n",
        "    # ================ ベースライン手法（GPT-4o込み） ================\n",
        "    print(\"ベースライン手法（従来特徴量 + GPT-4o）を実行中...\")\n",
        "\n",
        "    # SHAP分析で30個に絞る\n",
        "    X_baseline_all = train_data[baseline_features].fillna(0).values\n",
        "    y_train = train_data[\"unknownWordLabel\"].values\n",
        "\n",
        "    baseline_shap_features = extract_shap_features(X_baseline_all, y_train, baseline_features, n_features=30)\n",
        "    X_baseline = train_data[baseline_shap_features].fillna(0).values\n",
        "\n",
        "    # SBFS特徴量選択\n",
        "    X_baseline_selected, baseline_selected_features = feature_selection_sbs(\n",
        "        X_baseline, y_train, baseline_shap_features)\n",
        "\n",
        "    X_test_baseline = test_data[baseline_selected_features].fillna(0).values\n",
        "    y_test = test_data[\"unknownWordLabel\"].values\n",
        "\n",
        "    baseline_pred, baseline_prob = train_and_predict(\n",
        "        X_baseline_selected, y_train, X_test_baseline, y_test)\n",
        "\n",
        "    baseline_results['y_true'].extend(y_test)\n",
        "    baseline_results['y_pred'].extend(baseline_pred)\n",
        "    baseline_results['y_prob'].extend(baseline_prob[:, 1])\n",
        "    baseline_results['selected_features'].append(baseline_selected_features)\n",
        "\n",
        "    # ================ 提案手法（ベースライン + Transformer特徴量） ================\n",
        "    print(\"提案手法（ベースライン + Transformer視線特徴量）を実行中...\")\n",
        "\n",
        "    # SHAP分析で30個に絞る\n",
        "    X_proposed_all = train_data_with_features[proposed_features].fillna(0).values\n",
        "\n",
        "    proposed_shap_features = extract_shap_features(X_proposed_all, y_train, proposed_features, n_features=30)\n",
        "    X_proposed = train_data_with_features[proposed_shap_features].fillna(0).values\n",
        "\n",
        "    # SBFS特徴量選択\n",
        "    X_proposed_selected, proposed_selected_features = feature_selection_sbs(\n",
        "        X_proposed, y_train, proposed_shap_features)\n",
        "\n",
        "    X_test_proposed = test_data_with_features[proposed_selected_features].fillna(0).values\n",
        "\n",
        "    proposed_pred, proposed_prob = train_and_predict(\n",
        "        X_proposed_selected, y_train, X_test_proposed, y_test)\n",
        "\n",
        "    proposed_results['y_true'].extend(y_test)\n",
        "    proposed_results['y_pred'].extend(proposed_pred)\n",
        "    proposed_results['y_prob'].extend(proposed_prob[:, 1])\n",
        "    proposed_results['selected_features'].append(proposed_selected_features)\n",
        "\n",
        "    baseline_metrics = calculate_metrics(y_test, baseline_pred)\n",
        "    proposed_metrics = calculate_metrics(y_test, proposed_pred)\n",
        "\n",
        "    print(f\"ベースライン (GPT-4o) - F1: {baseline_metrics['f1_score']:.3f}\")\n",
        "    print(f\"提案手法 (GPT-4o + Transformer) - F1: {proposed_metrics['f1_score']:.3f}\")\n",
        "\n",
        "# ================ 全体結果の分析 ================\n",
        "print(\"\\n=== 全体結果の分析 ===\")\n",
        "\n",
        "for key in ['y_true', 'y_pred', 'y_prob']:\n",
        "    baseline_results[key] = np.array(baseline_results[key])\n",
        "    proposed_results[key] = np.array(proposed_results[key])\n",
        "\n",
        "baseline_overall = calculate_metrics(baseline_results['y_true'], baseline_results['y_pred'])\n",
        "proposed_overall = calculate_metrics(proposed_results['y_true'], proposed_results['y_pred'])\n",
        "\n",
        "print(\"=== ベースライン手法（従来 + GPT-4o） ===\")\n",
        "print(f\"F1-score: {baseline_overall['f1_score']:.3f}\")\n",
        "print(f\"Accuracy: {baseline_overall['accuracy']:.3f}\")\n",
        "print(f\"Precision: {baseline_overall['precision']:.3f}\")\n",
        "print(f\"Recall: {baseline_overall['recall']:.3f}\")\n",
        "\n",
        "print(\"\\n=== 提案手法（ベースライン + Transformer視線情報） ===\")\n",
        "print(f\"F1-score: {proposed_overall['f1_score']:.3f}\")\n",
        "print(f\"Accuracy: {proposed_overall['accuracy']:.3f}\")\n",
        "print(f\"Precision: {proposed_overall['precision']:.3f}\")\n",
        "print(f\"Recall: {proposed_overall['recall']:.3f}\")\n",
        "\n",
        "print(f\"\\n=== Transformer視線特徴量の効果 ===\")\n",
        "print(f\"F1スコア改善: {proposed_overall['f1_score'] - baseline_overall['f1_score']:+.3f}\")\n",
        "\n",
        "improvement_analysis = analyze_improvement(baseline_results, proposed_results)\n",
        "\n",
        "print(f\"\\n新たに正しく分類された単語数: {improvement_analysis['newly_correct_count']}\")\n",
        "print(f\"新たに間違って分類された単語数: {improvement_analysis['newly_incorrect_count']}\")\n",
        "print(f\"純改善数: {improvement_analysis['net_improvement']}\")\n",
        "\n",
        "def get_difficulty_level(score):\n",
        "    \"\"\"難易度スコアを文字レベルに変換\"\"\"\n",
        "    if score <= 0.2:\n",
        "        return \"Very Easy (1-2)\"\n",
        "    elif score <= 0.4:\n",
        "        return \"Easy (3-4)\"\n",
        "    elif score <= 0.6:\n",
        "        return \"Moderate (5-6)\"\n",
        "    elif score <= 0.8:\n",
        "        return \"Difficult (7-8)\"\n",
        "    else:\n",
        "        return \"Very Difficult (9-10)\"\n",
        "\n",
        "# ================ 単語レベル詳細分析の保存 ================\n",
        "print(\"\\n単語レベルの詳細分析を保存中...\")\n",
        "\n",
        "word_level_analysis = []\n",
        "word_index = 0\n",
        "\n",
        "for test_doc in documents:\n",
        "    test_data = df_with_gpt[df_with_gpt[\"id_document\"] == test_doc].copy()\n",
        "\n",
        "    for _, row in test_data.iterrows():\n",
        "        word_info = {\n",
        "            'word_index': word_index,\n",
        "            'document_id': int(row['id_document']),\n",
        "            'line_id': int(row['id_line']),\n",
        "            'word_id': int(row['id_word']),\n",
        "            'word': row['word'],\n",
        "            'context': row['line'],  # Lineを文脈として使用\n",
        "            'gpt_difficulty': float(row['gpt_difficulty']),\n",
        "            'gpt_difficulty_integer': int(row['gpt_difficulty'] * 10),\n",
        "            'difficulty_level': get_difficulty_level(float(row['gpt_difficulty'])),\n",
        "            'true_label': int(baseline_results['y_true'][word_index]),\n",
        "            'baseline_prediction': int(baseline_results['y_pred'][word_index]),\n",
        "            'proposed_prediction': int(proposed_results['y_pred'][word_index]),\n",
        "            'baseline_probability': float(baseline_results['y_prob'][word_index]),\n",
        "            'proposed_probability': float(proposed_results['y_prob'][word_index]),\n",
        "            'baseline_correct': bool(baseline_results['y_true'][word_index] == baseline_results['y_pred'][word_index]),\n",
        "            'proposed_correct': bool(proposed_results['y_true'][word_index] == proposed_results['y_pred'][word_index]),\n",
        "        }\n",
        "\n",
        "        if not word_info['baseline_correct'] and word_info['proposed_correct']:\n",
        "            word_info['improvement_category'] = 'newly_correct'\n",
        "        elif word_info['baseline_correct'] and not word_info['proposed_correct']:\n",
        "            word_info['improvement_category'] = 'newly_incorrect'\n",
        "        elif word_info['baseline_correct'] and word_info['proposed_correct']:\n",
        "            word_info['improvement_category'] = 'both_correct'\n",
        "        else:\n",
        "            word_info['improvement_category'] = 'both_incorrect'\n",
        "\n",
        "        word_level_analysis.append(word_info)\n",
        "        word_index += 1\n",
        "\n",
        "# 新しく正解した単語のリスト作成（難易度順）\n",
        "newly_correct_words = [w for w in word_level_analysis if w['improvement_category'] == 'newly_correct']\n",
        "newly_correct_words.sort(key=lambda x: x['gpt_difficulty'], reverse=True)\n",
        "\n",
        "# 新しく誤判定した単語のリスト作成（難易度順）\n",
        "newly_incorrect_words = [w for w in word_level_analysis if w['improvement_category'] == 'newly_incorrect']\n",
        "newly_incorrect_words.sort(key=lambda x: x['gpt_difficulty'], reverse=True)\n",
        "\n",
        "# 新しく正解した単語の詳細ファイル保存\n",
        "with open(f'Output/GPT_Transformer/analysis/words/newly_correct_words_{experiment_number}.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"新しく正しく分類された単語一覧 (実験番号: {experiment_number})\\n\")\n",
        "    f.write(f\"総数: {len(newly_correct_words)} 単語\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    for i, word_info in enumerate(newly_correct_words, 1):\n",
        "        f.write(f\"[{i}] 単語: {word_info['word']}\\n\")\n",
        "        f.write(f\"    文脈: {word_info['context']}\\n\")\n",
        "        f.write(f\"    GPT-4o難易度スコア: {word_info['gpt_difficulty_integer']}/10 ({word_info['difficulty_level']})\\n\")\n",
        "        f.write(f\"    真のラベル: {'未知' if word_info['true_label']==1 else '既知'}\\n\")\n",
        "        f.write(f\"    ベースライン予測: {'未知' if word_info['baseline_prediction']==1 else '既知'}\\n\")\n",
        "        f.write(f\"    提案手法予測: {'未知' if word_info['proposed_prediction']==1 else '既知'}\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "# 新しく誤判定した単語の詳細ファイル保存\n",
        "with open(f'Output/GPT_Transformer/analysis/words/newly_incorrect_words_{experiment_number}.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"新しく誤って分類された単語一覧 (実験番号: {experiment_number})\\n\")\n",
        "    f.write(f\"総数: {len(newly_incorrect_words)} 単語\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    for i, word_info in enumerate(newly_incorrect_words, 1):\n",
        "        f.write(f\"[{i}] 単語: {word_info['word']}\\n\")\n",
        "        f.write(f\"    文脈: {word_info['context']}\\n\")\n",
        "        f.write(f\"    GPT-4o難易度スコア: {word_info['gpt_difficulty_integer']}/10 ({word_info['difficulty_level']})\\n\")\n",
        "        f.write(f\"    真のラベル: {'未知' if word_info['true_label']==1 else '既知'}\\n\")\n",
        "        f.write(f\"    ベースライン予測: {'未知' if word_info['baseline_prediction']==1 else '既知'}\\n\")\n",
        "        f.write(f\"    提案手法予測: {'未知' if word_info['proposed_prediction']==1 else '既知'}\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "print(f\"単語詳細ファイルを保存しました:\")\n",
        "print(f\"  - 新しく正解: Output/GPT_Transformer/analysis/words/newly_correct_words_{experiment_number}.txt\")\n",
        "print(f\"  - 新しく誤判定: Output/GPT_Transformer/analysis/words/newly_incorrect_words_{experiment_number}.txt\")\n",
        "\n",
        "# ================ 結果の可視化（個別画像保存） ================\n",
        "print(\"\\n結果を可視化中...\")\n",
        "\n",
        "# GPT-4o難易度の分布を確認\n",
        "gpt_scores = df_with_gpt['gpt_difficulty'].values\n",
        "unknown_scores = gpt_scores[df_with_gpt['unknownWordLabel'] == 1]\n",
        "known_scores = gpt_scores[df_with_gpt['unknownWordLabel'] == 0]\n",
        "\n",
        "# PR曲線の計算\n",
        "baseline_precision, baseline_recall, _ = precision_recall_curve(\n",
        "    baseline_results['y_true'], baseline_results['y_prob'])\n",
        "proposed_precision, proposed_recall, _ = precision_recall_curve(\n",
        "    proposed_results['y_true'], proposed_results['y_prob'])\n",
        "\n",
        "def interpolate_precision_recall(precision, recall):\n",
        "    precision_interp = []\n",
        "    for i in range(11):\n",
        "        recall_level = i / 10.0\n",
        "        max_precision = 0.0\n",
        "        for j, r in enumerate(recall):\n",
        "            if r >= recall_level and precision[j] > max_precision:\n",
        "                max_precision = precision[j]\n",
        "        precision_interp.append(max_precision)\n",
        "    return precision_interp, [i/10.0 for i in range(11)]\n",
        "\n",
        "baseline_prec_interp, recall_interp = interpolate_precision_recall(baseline_precision, baseline_recall)\n",
        "proposed_prec_interp, _ = interpolate_precision_recall(proposed_precision, proposed_recall)\n",
        "\n",
        "# 個別画像1: PR曲線\n",
        "fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
        "ax1.plot(recall_interp, proposed_prec_interp, 'b-o',\n",
        "         label=f'Proposed (AUC={auc(recall_interp, proposed_prec_interp):.3f})', markersize=4)\n",
        "ax1.plot(recall_interp, baseline_prec_interp, 'r-o',\n",
        "         label=f'Baseline (AUC={auc(recall_interp, baseline_prec_interp):.3f})', markersize=4)\n",
        "ax1.set_xlabel('Recall')\n",
        "ax1.set_ylabel('Precision')\n",
        "ax1.set_title(f'PR Curve (GPT-4o+Transformer) - Exp {experiment_number}')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'Output/GPT_Transformer/aupr/individual/PR_curve_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# 個別画像2: F1スコア比較\n",
        "fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
        "ax2.bar(['Baseline\\n(従来+GPT-4o)', 'Proposed\\n(+Transformer)'],\n",
        "        [baseline_overall['f1_score'], proposed_overall['f1_score']],\n",
        "        color=['red', 'blue'], alpha=0.7)\n",
        "ax2.set_ylabel('F1-Score')\n",
        "ax2.set_title(f'F1 Comparison - Exp {experiment_number}')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "# F1スコア改善を表示\n",
        "improvement = proposed_overall['f1_score'] - baseline_overall['f1_score']\n",
        "if improvement > 0:\n",
        "    ax2.text(1, proposed_overall['f1_score'], f'+{improvement:.3f}',\n",
        "             ha='center', va='bottom', fontsize=10, color='blue')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'Output/GPT_Transformer/aupr/individual/F1_comparison_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# 個別画像3: 改善分析\n",
        "fig3, ax3 = plt.subplots(figsize=(8, 6))\n",
        "categories = ['Newly Correct', 'Newly Incorrect', 'Net Improvement']\n",
        "values = [improvement_analysis['newly_correct_count'],\n",
        "          improvement_analysis['newly_incorrect_count'],\n",
        "          improvement_analysis['net_improvement']]\n",
        "colors = ['green', 'red', 'blue']\n",
        "\n",
        "bars = ax3.bar(categories, values, color=colors, alpha=0.7)\n",
        "ax3.set_ylabel('Number of Words')\n",
        "ax3.set_title(f'Impact (GPT-4o+Transformer) - Exp {experiment_number}')\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, value in zip(bars, values):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "             f'{value}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'Output/GPT_Transformer/aupr/individual/improvement_analysis_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# 個別画像4: GPT-4o難易度分布比較\n",
        "fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
        "ax4.hist(known_scores, bins=10, alpha=0.7,\n",
        "         label=f'Known Words (n={len(known_scores)})', color='blue', density=True)\n",
        "ax4.hist(unknown_scores, bins=10, alpha=0.7,\n",
        "         label=f'Unknown Words (n={len(unknown_scores)})', color='red', density=True)\n",
        "ax4.set_xlabel('GPT-4o Difficulty Score')\n",
        "ax4.set_ylabel('Density')\n",
        "ax4.set_title(f'Difficulty Distribution - Exp {experiment_number}')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'Output/GPT_Transformer/aupr/individual/difficulty_distribution_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"個別画像を保存しました:\")\n",
        "print(f\"  - Output/GPT_Transformer/aupr/individual/PR_curve_{experiment_number}.png\")\n",
        "print(f\"  - Output/GPT_Transformer/aupr/individual/F1_comparison_{experiment_number}.png\")\n",
        "print(f\"  - Output/GPT_Transformer/aupr/individual/improvement_analysis_{experiment_number}.png\")\n",
        "print(f\"  - Output/GPT_Transformer/aupr/individual/difficulty_distribution_{experiment_number}.png\")\n",
        "\n",
        "# ================ 結果の保存 ================\n",
        "print(\"\\n結果を保存中...\")\n",
        "\n",
        "with open(f'Output/GPT_Transformer/selected_features/baseline_features_{experiment_number}.pkl', 'wb') as f:\n",
        "    pickle.dump(baseline_results['selected_features'], f)\n",
        "\n",
        "with open(f'Output/GPT_Transformer/selected_features/proposed_features_{experiment_number}.pkl', 'wb') as f:\n",
        "    pickle.dump(proposed_results['selected_features'], f)\n",
        "\n",
        "analysis_results = {\n",
        "    'experiment_number': experiment_number,\n",
        "    'baseline_results': convert_numpy_to_list(baseline_overall),\n",
        "    'proposed_results': convert_numpy_to_list(proposed_overall),\n",
        "    'improvement_analysis': convert_numpy_to_list(improvement_analysis),\n",
        "    'word_level_analysis': word_level_analysis\n",
        "}\n",
        "\n",
        "with open(f'Output/GPT_Transformer/analysis/complete_analysis_{experiment_number}.json', 'w') as f:\n",
        "    json.dump(analysis_results, f, indent=2)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"処理完了！ 総実行時間: {elapsed_time/60:.2f} 分\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n結果保存先: Output/GPT_Transformer/\")\n",
        "print(f\"  - aupr/individual/ : 個別画像\")\n",
        "print(f\"  - analysis/words/ : 単語詳細\")\n",
        "print(f\"  - analysis/ : 分析結果JSON\")\n",
        "print(f\"  - selected_features/ : 選択特徴量\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "wGCNLQFi9y5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd1d7879-29ba-4f68-b8c8-0d308f321333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "実験番号 c07 のGPT-4o + Transformer難易度評価分析を開始します...\n",
            "✅ OpenAI APIクライアントが正常に初期化されました。\n",
            "使用モデル: gpt-4o + Transformer\n",
            "処理開始時刻: 2025-12-24 19:11:19\n",
            "データを読み込んでいます...\n",
            "データ形状: (2202, 31)\n",
            "未知単語ラベルの分布: Counter({0: 2117, 1: 85})\n",
            "\n",
            "lineカラムから文脈データを取得中...\n",
            "文脈取得完了: 2202 単語\n",
            "\n",
            "特徴量構成:\n",
            "  従来特徴量: 22個\n",
            "  ベースライン (従来 + GPT-4o): 23個\n",
            "  提案手法 (ベースライン + Transformer): 55個\n",
            "\n",
            "GPT-4o難易度特徴量を追加中...\n",
            "gpt-4oによる難易度評価を実行中（新規評価）...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2202/2202 [15:10<00:00,  2.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "文書別クロスバリデーション開始...\n",
            "\n",
            "=== 文書 0 をテストデータとして使用 ===\n",
            "訓練データ: 1953 サンプル\n",
            "テストデータ: 249 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 77\n",
            "  Epoch 2/10, Loss: 0.2133\n",
            "  Epoch 4/10, Loss: 0.2114\n",
            "  Epoch 6/10, Loss: 0.2090\n",
            "  Epoch 8/10, Loss: 0.2074\n",
            "  Epoch 10/10, Loss: 0.2049\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 11\n",
            "選択された特徴量: ['gpt_difficulty', 'Num_7Characters', 'Rate_7Characters', 'freq', 'Num_Polysyllable', 'Num_ContentWords', 'Length_Word_Ave', 'Rate_ContentWords', 'Num_Monosyllable', 'Num_FunctionWords', 'freq_Min']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 12\n",
            "選択された特徴量: ['transformer30', 'transformer19', 'freq', 'transformer20', 'transformer21', 'transformer23', 'Num_7Characters', 'transformer24', 'Num_Polysyllable', 'transformer28', 'Num_Words', 'Length_Word_Ave']\n",
            "ベースライン (GPT-4o) - F1: 0.368\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.382\n",
            "\n",
            "=== 文書 1 をテストデータとして使用 ===\n",
            "訓練データ: 1872 サンプル\n",
            "テストデータ: 330 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 75\n",
            "  Epoch 2/10, Loss: 0.3143\n",
            "  Epoch 4/10, Loss: 0.3076\n",
            "  Epoch 6/10, Loss: 0.3020\n",
            "  Epoch 8/10, Loss: 0.2974\n",
            "  Epoch 10/10, Loss: 0.2930\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 1\n",
            "選択された特徴量: ['gpt_difficulty']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 7\n",
            "選択された特徴量: ['freq', 'transformer20', 'transformer6', 'transformer27', 'transformer28', 'transformer17', 'transformer25']\n",
            "ベースライン (GPT-4o) - F1: 0.188\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.327\n",
            "\n",
            "=== 文書 2 をテストデータとして使用 ===\n",
            "訓練データ: 1933 サンプル\n",
            "テストデータ: 269 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 77\n",
            "  Epoch 2/10, Loss: 0.3104\n",
            "  Epoch 4/10, Loss: 0.3022\n",
            "  Epoch 6/10, Loss: 0.2978\n",
            "  Epoch 8/10, Loss: 0.2940\n",
            "  Epoch 10/10, Loss: 0.2890\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 13\n",
            "選択された特徴量: ['freq', 'Length_Word_Ave', 'Num_Polysyllable', 'Num_Words', 'Num_ContentWords', 'freq_Min', 'freq_Ave', 'Rate_7Characters', 'Num_FunctionWords', 'Rate_ContentWords', 'Rate_FunctionWords', 'Num_Monosyllable', 'freq_Max']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 10\n",
            "選択された特徴量: ['transformer20', 'freq', 'Length_Word_Ave', 'transformer1', 'transformer11', 'Num_Polysyllable', 'transformer28', 'freq_Min', 'transformer21', 'Num_7Characters']\n",
            "ベースライン (GPT-4o) - F1: 0.353\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.324\n",
            "\n",
            "=== 文書 3 をテストデータとして使用 ===\n",
            "訓練データ: 1913 サンプル\n",
            "テストデータ: 289 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 77\n",
            "  Epoch 2/10, Loss: 0.3124\n",
            "  Epoch 4/10, Loss: 0.3060\n",
            "  Epoch 6/10, Loss: 0.3006\n",
            "  Epoch 8/10, Loss: 0.2961\n",
            "  Epoch 10/10, Loss: 0.2900\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 9\n",
            "選択された特徴量: ['gpt_difficulty', 'freq', 'Num_Monosyllable', 'Length_Word_Ave', 'Rate_7Characters', 'Num_ContentWords', 'Num_Polysyllable', 'Num_FunctionWords', 'Rate_ContentWords']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 16\n",
            "選択された特徴量: ['transformer28', 'transformer12', 'gpt_difficulty', 'transformer5', 'freq', 'transformer16', 'transformer17', 'transformer14', 'transformer21', 'transformer19', 'transformer23', 'transformer10', 'Num_Monosyllable', 'transformer6', 'transformer15', 'Num_7Characters']\n",
            "ベースライン (GPT-4o) - F1: 0.308\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.235\n",
            "\n",
            "=== 文書 4 をテストデータとして使用 ===\n",
            "訓練データ: 1941 サンプル\n",
            "テストデータ: 261 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 77\n",
            "  Epoch 2/10, Loss: 0.3100\n",
            "  Epoch 4/10, Loss: 0.3046\n",
            "  Epoch 6/10, Loss: 0.2998\n",
            "  Epoch 8/10, Loss: 0.2978\n",
            "  Epoch 10/10, Loss: 0.2952\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 8\n",
            "選択された特徴量: ['freq', 'Num_ContentWords', 'Length_Word_Ave', 'Num_Words', 'Num_Polysyllable', 'freq_Min', 'freq_Ave', 'ReadBack']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 5\n",
            "選択された特徴量: ['freq', 'transformer22', 'Num_Polysyllable', 'transformer14', 'Num_ContentWords']\n",
            "ベースライン (GPT-4o) - F1: 0.178\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.255\n",
            "\n",
            "=== 文書 5 をテストデータとして使用 ===\n",
            "訓練データ: 1940 サンプル\n",
            "テストデータ: 262 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 77\n",
            "  Epoch 2/10, Loss: 0.3088\n",
            "  Epoch 4/10, Loss: 0.3002\n",
            "  Epoch 6/10, Loss: 0.2933\n",
            "  Epoch 8/10, Loss: 0.2873\n",
            "  Epoch 10/10, Loss: 0.2795\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 23\n",
            "選択された特徴量: ['gpt_difficulty', 'freq', 'Length_Word_Ave', 'Num_7Characters', 'Num_Monosyllable', 'Num_ContentWords', 'Num_Polysyllable', 'Num_Words', 'syllables', 'Rate_7Characters', 'ContentWord', 'Readtime', 'Flesch_Reading_Ease', 'seven_character', 'Rate_ContentWords', 'Rate_FunctionWords', 'freq_Min', 'length', 'Num_FunctionWords', 'freq_Ave', 'freq_Max', 'ReadBack', 'ARI']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 23\n",
            "選択された特徴量: ['transformer5', 'transformer24', 'Length_Word_Ave', 'transformer11', 'transformer4', 'freq', 'transformer0', 'transformer6', 'transformer25', 'transformer13', 'transformer2', 'transformer26', 'transformer1', 'transformer29', 'transformer31', 'transformer14', 'transformer7', 'transformer3', 'Num_Words', 'transformer15', 'Num_7Characters', 'transformer16', 'Num_ContentWords']\n",
            "ベースライン (GPT-4o) - F1: 0.171\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.146\n",
            "\n",
            "=== 文書 6 をテストデータとして使用 ===\n",
            "訓練データ: 1937 サンプル\n",
            "テストデータ: 265 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 77\n",
            "  Epoch 2/10, Loss: 0.3133\n",
            "  Epoch 4/10, Loss: 0.3054\n",
            "  Epoch 6/10, Loss: 0.3010\n",
            "  Epoch 8/10, Loss: 0.2969\n",
            "  Epoch 10/10, Loss: 0.2926\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 10\n",
            "選択された特徴量: ['freq', 'Length_Word_Ave', 'Num_7Characters', 'Num_Polysyllable', 'Num_Words', 'freq_Min', 'Num_FunctionWords', 'Rate_FunctionWords', 'Rate_ContentWords', 'freq_Max']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 8\n",
            "選択された特徴量: ['freq', 'transformer28', 'transformer6', 'transformer14', 'Length_Word_Ave', 'transformer13', 'Num_Polysyllable', 'Rate_7Characters']\n",
            "ベースライン (GPT-4o) - F1: 0.286\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.308\n",
            "\n",
            "=== 文書 7 をテストデータとして使用 ===\n",
            "訓練データ: 1925 サンプル\n",
            "テストデータ: 277 サンプル\n",
            "Transformerモデルを訓練中...\n",
            "訓練データの準備中...\n",
            "  ユニークなline_id数: 77\n",
            "  Epoch 2/10, Loss: 0.3098\n",
            "  Epoch 4/10, Loss: 0.3036\n",
            "  Epoch 6/10, Loss: 0.2997\n",
            "  Epoch 8/10, Loss: 0.2941\n",
            "  Epoch 10/10, Loss: 0.2863\n",
            "Transformer特徴量を抽出中...\n",
            "ベースライン手法（従来特徴量 + GPT-4o）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 23個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 8\n",
            "選択された特徴量: ['freq', 'Num_ContentWords', 'Num_Words', 'freq_Ave', 'Num_Polysyllable', 'Rate_ContentWords', 'Rate_FunctionWords', 'ReadBack']\n",
            "提案手法（ベースライン + Transformer視線特徴量）を実行中...\n",
            "SHAP分析により上位30特徴量を選択中...\n",
            "SHAP選択完了: 30個の特徴量\n",
            "Sequential Backward Selection (SBS)実行中...\n",
            "選択された特徴量数: 9\n",
            "選択された特徴量: ['transformer3', 'freq', 'transformer6', 'Num_7Characters', 'transformer30', 'transformer10', 'transformer21', 'transformer17', 'Num_Monosyllable']\n",
            "ベースライン (GPT-4o) - F1: 0.258\n",
            "提案手法 (GPT-4o + Transformer) - F1: 0.238\n",
            "\n",
            "=== 全体結果の分析 ===\n",
            "=== ベースライン手法（従来 + GPT-4o） ===\n",
            "F1-score: 0.264\n",
            "Accuracy: 0.838\n",
            "Precision: 0.160\n",
            "Recall: 0.753\n",
            "\n",
            "=== 提案手法（ベースライン + Transformer視線情報） ===\n",
            "F1-score: 0.284\n",
            "Accuracy: 0.858\n",
            "Precision: 0.177\n",
            "Recall: 0.729\n",
            "\n",
            "=== Transformer視線特徴量の効果 ===\n",
            "F1スコア改善: +0.020\n",
            "\n",
            "新たに正しく分類された単語数: 135\n",
            "新たに間違って分類された単語数: 91\n",
            "純改善数: 44\n",
            "\n",
            "単語レベルの詳細分析を保存中...\n",
            "単語詳細ファイルを保存しました:\n",
            "  - 新しく正解: Output/GPT_Transformer/analysis/words/newly_correct_words_c07.txt\n",
            "  - 新しく誤判定: Output/GPT_Transformer/analysis/words/newly_incorrect_words_c07.txt\n",
            "\n",
            "結果を可視化中...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-452503940.py:659: UserWarning: Glyph 24467 (\\N{CJK UNIFIED IDEOGRAPH-5F93}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-452503940.py:659: UserWarning: Glyph 26469 (\\N{CJK UNIFIED IDEOGRAPH-6765}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-452503940.py:660: UserWarning: Glyph 24467 (\\N{CJK UNIFIED IDEOGRAPH-5F93}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(f'Output/GPT_Transformer/aupr/individual/F1_comparison_{experiment_number}.png', dpi=300, bbox_inches='tight')\n",
            "/tmp/ipython-input-452503940.py:660: UserWarning: Glyph 26469 (\\N{CJK UNIFIED IDEOGRAPH-6765}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(f'Output/GPT_Transformer/aupr/individual/F1_comparison_{experiment_number}.png', dpi=300, bbox_inches='tight')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "個別画像を保存しました:\n",
            "  - Output/GPT_Transformer/aupr/individual/PR_curve_c07.png\n",
            "  - Output/GPT_Transformer/aupr/individual/F1_comparison_c07.png\n",
            "  - Output/GPT_Transformer/aupr/individual/improvement_analysis_c07.png\n",
            "  - Output/GPT_Transformer/aupr/individual/difficulty_distribution_c07.png\n",
            "\n",
            "結果を保存中...\n",
            "\n",
            "============================================================\n",
            "処理完了！ 総実行時間: 154.81 分\n",
            "============================================================\n",
            "\n",
            "結果保存先: Output/GPT_Transformer/\n",
            "  - aupr/individual/ : 個別画像\n",
            "  - analysis/words/ : 単語詳細\n",
            "  - analysis/ : 分析結果JSON\n",
            "  - selected_features/ : 選択特徴量\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}